# 6个常见优化算法

> 作业部落：https://www.zybuluo.com/HolyShine/note/1529283

今天来讨论下常见的几个优化算法。Momentum、Nesterov Accelerated Gradient、AdaGrad、AdaDelta、RMSprop和Adam。

我们知道最朴素的 SGD 优化器采用的优化策略是：
$$
\tag 1
\theta_{t} = \theta_{t-1} - \alpha g(\theta_{t-1})
$$
其中 $g(\theta_{t-1})$ 表示损失在当前点的梯度。  

其他优化算法都是在此基础上改进的

## 1. Momentum

从式(1)可以看到，每次都是对参数更新一个量 $g(\theta)$，$\alpha$ 来控制这个更新的度。

Momentum的想法很简单，这个参数的更新方向不仅与当前损失对参数的梯度有关，还受到上一次参数更新方向的影响，其更新公式如下：
$$
\tag 2
\begin{array}{}
d_t = \beta d_{t-1}+g(\theta_{t-1})\\
\theta_t = \theta_{t-1}-\alpha d_t
\end{array}
$$
这里用 $d_i$ 保存这一次的梯度更新方向，在后续的参数更新过程中，用它乘以一个衰减系数 $\beta$ 来作用参数的更新方向。  

这个动量梯度的物理意义也很显然，**小球在山谷里滚落的时候，会收到之前历史动量的影响**。  

用一张图直观地来解释这种方法的优势：

<img src='http://image100.360doc.com/DownloadImg/2016/10/1008/81923986_10.jpg'>

左图是SGD，右图是冲量的SGD，可以看到使用冲量梯度的效果就是梯度下降在每个拐点不会改变地很极端，更平滑，减缓了抖动，使得模型能够更平稳、快速地冲向局部最优点。



## 2. Nesterov Accelerated Gradient

NAG，N式加速梯度法（Nesterov 是发明者的名字）。

合并动量梯度式子(2)，我们得到：
$$
\tag 3
\theta_t = \theta_{t-1}-\alpha\beta d_{t-1}-g(\theta_{t-1})
$$
发现参数每次在更新的时候，都要先走一个 $\alpha\beta d_{t-1}$ 的量。

那么为什么不直接先走到这个位置，再用这个位置的梯度来更新参数呢？

基于这个想法，Nesterov提出了他的梯度更新公式：
$$
\tag 4
\begin{array}{}
d_{t} = \beta d_{t-1}+g(\theta_{t-1}-\alpha\beta d_{t-1})\\
\theta_t = \theta_{t-1}-\alpha d_t
\end{array}
$$
这与动量梯度的唯一区别是：NAG的梯度是先让参数走了计划要走的一步后，再计算出来的，而不是在当前点直接计算的。

这个往前看**本质上是利用了目标函数的二阶导信息**：如果这次的梯度比上次的梯度变大，那么有理由相信它会继续变大下去，于是就可以把预计要增大的部分提前加进来。

>NAG在梯度更新的时候以“向前看”看到的梯度而不是当前位置的梯度去更新。经过变换之后的等效形式中，NAG算法相对于Momentum多了一个本次梯度相对上次梯度的变化量，这个变化量本质上是对目标函数二阶导的近似。利用二阶导加速了收敛



## 3. AdaGrad

AdaGradtongy同样是解决收敛过程的震荡问题。  

先来看下它的梯度更新公式：
$$
\tag 5
\theta_t = \theta_{t-1}-\frac{\alpha}{\sqrt{\sum_{i=1}^{t-1} g(\theta_i)^2}+\epsilon }g(\theta_{t-1})
$$

- 局部来说，AdaGrad会在惩罚梯度较大的参数维度，使得这部分更新更缓慢。即对更新频率低的参数，提供更大更新，对更新频率高的参数，提供小更新。
- 总体来说，分母部分会越来越大，AdaGrad使得参数的更新越来越缓慢，因此会出现学习率过度衰减的问题。



## 4. AdaDelta和RMSprop

AdaDelta和RMSprop事实上是一种东西。都是为了解决AdaGrad的学习率过度衰减问题。

与AdaGrad使用历史梯度的累积平方和作为衰减因子不同，这两种方法使用了梯度平方的**指数加权平均**。它们的更新公式如下：
$$
\tag 6
\begin{array}{}
E[g^2]_t &= \gamma E[g^2]_{t-1}+(1-\gamma)g^2_{t-1}\\
\theta_t &=\theta_{t-1}-\frac{\alpha}{\sqrt{E[g^2]_t+\epsilon}}g_{t-1}
\end{array}
$$
一般取 $\gamma=0.9$。

可以看到这个衰减可以稳定在一个较为合理的值



## 5. Adam

Adam同时考虑了动量和梯度衰减，即估计了梯度的一阶矩（梯度指数加权平均）和二阶矩（梯度平方指数加权平均）：
$$
\tag 7
\begin{array}{l}
m_t &=\beta_1 m_{t-1}+(1-\beta_1)g_t\\
v_t &=\beta_2 v_{t-1}+(1-\beta_2)g_t^2 
\end{array}
$$
但这里有个问题，由于 $m_t, v_t$ 初始化为0，因此更新的初始阶段，$m_t, v_t$ 很难有所进展，因此加入 bias-correct：
$$
\tag 8
\begin{array}{}
\hat m_t &= \frac{m_t}{1-\beta^t_1}\\
\hat v_t &=\frac{v_t}{1-\beta^t_2} 
\end{array}
$$
一般取 $\beta_1= 0.9, \beta_2=0.999$.

第一轮 $t=1$，则 $\hat m_t$ 相比 $m_t$ 会放大，后期分母慢慢接近于1。

因此整个参数更新过程如下：
$$
\tag 9
\theta_t = \theta_{t-1}-\frac{\alpha}{\sqrt{\hat v_t+\epsilon}}\hat m_t
$$


## 6. 总结

总结一下这些优化算法的发展脉络：

![](imgs\summary.svg)

总的来说，上面这条线是通过动量梯度来加快大梯度方向的收敛，下面这条线是减缓小梯度方向的抖动，它们都能加速收敛速度。

## 参考资料

[比Momentum更快: 解开Nesterov Accelerated Gradient的真面目](http://www.360doc.com/content/16/1010/08/36492363_597225745.shtml)

[从 SGD 到 Adam―― 6大常见优化算法总结](https://zhuanlan.zhihu.com/p/64113429)

