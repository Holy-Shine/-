# 多种反向传播的推导

>作业部落地址：https://www.zybuluo.com/HolyShine/note/1527507

## 1. Softmax+CrossEntropy

假定最后一层`softmaxt` 层之前有 $N$ 个输出，每个输出为 $\{z_i\}_{i=1}^N$，$N$ 个输出经过 `softmax` 后的输出（即最后的概率输出）为 $\{S(z_i)\}_{i=1}^N$，有：
$$
\begin{align}
S(z_i)=\frac{e^{z_i}}{\sum_{j=1}^N e^{z_i}}
\end{align}
$$
网络上有张非常容易理解的`softmax` 的图解：

<center><img src='imgs\softmax.jpg' width=70%></center>
<center><b>图1. softmax结构</b></center>
后接交叉熵损失：
$$
\begin{align}
L_{CE}=\sum_{i=1}^N -y_i\log S(z_i)
\end{align}
$$
根据链式规则, 以下 $S(z_i)$ 简写为 $S_i$：
$$
\begin{align}
\frac{\partial L_{CE}}{\partial z_k}&=-\sum_{i=1}^N y_i\frac{\partial \log S_i}{\partial z_k}\\
&=-y_k\frac{1}{S_k}\frac{\partial S_k}{\partial z_k}-\sum_{i!=k}y_i\frac{1}{S_i}\frac{\partial S_i}{\partial z_k}\\

\end{align}
$$
上式分成了两个部分，$i=k$ 和 $i!=k$ ，求解的关键是，在这两种情况下，$\frac{\partial S_i}{\partial z_k}$ 分别是什么。

根据 `softmax` 的公式，

若 $i=k$：
$$
\begin{align}
\frac{\partial S_k}{\partial z_k}&=\frac{e^{z_k}\sum_{j=1}^Ne^{z_j}-e^{z_k}e^{z_k}}{\left(\sum_{j=1}^N e^{z_j}\right)^2}\\
&=\frac{e^{z_k}}{\sum_{j=1}^N e^{z_j}}\left(1-\frac{e^{z_k}}{\sum_{j=1}^N e^{z_j}}\right)\\
&= S_k\cdot(1-S_k)
\end{align}
$$
若 $i!=k$：
$$
\begin{align}
\frac{\partial S_i}{\partial z_k}&=-\frac{e^{z_i}e^{z_k}}{\left(\sum_{j=1}^N e^{z_j}\right)^2}\\
&= -S_i\cdot S_k
\end{align}
$$
可以看到形式非常简单，带入式(6)：
$$
\begin{align}
\frac{\partial L_{CE}}{\partial z_k}
&=-y_k\frac{1}{S_k}\frac{\partial S_k}{\partial z_k}-\sum_{i!=k}y_i\frac{1}{S_i}\frac{\partial S_i}{\partial z_k}\\
&=-y_k\frac{1}{S_k}S_k\cdot(1-S_k)-\sum_{i!=k}y_i\frac{1}{S_i}\cdot(-S_i\cdot S_k)\\
&=-y_k(1-S_k)+\sum_{i!=k}y_iS_k\\
&=y_kS_k+\sum_{i!=k}y_iS_k-y_k\\
&=\sum_{i=1}^Ny_iS_k -y_k\\
&=S_k-y_k
\end{align}
$$
从中可以发现交叉熵有着很好的偏导特性



## 2. MLP

有如下的3层神经网络：

<center><img src='imgs\MLP.PNG' width=80%></center>
<center><b>图2. 3层全连接</b></center>
根据上图，有如下定义：

- $\mathcal{L} = \frac{1}{2}\vert y- z\vert^2$，假定损失为均方误差损失

- $z_k = f(\sum_{j=1}^hw_{kj}y_j )$
- $\text{net}_k = \sum_{j=1}^hw_{kj}y_j$
- $y_j = f(\sum_{i=1}^d w_{ji}x_i)$
- $\text{net}_j = \sum_{i=1}^d w_{ji}x_i$
- $x_i$ 表示输入$x$ 的第 $i$ 个维度的值 

根据链式规则，隐层到输出层的权值偏导：
$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial w_{kj}} &= (y_k-z_k)\cdot-1\cdot\frac{\partial z_k}{\partial w_{kj}}\\
&=-(y_k-z_k)\cdot \frac{\partial z_k}{\partial \text{net}_k}\cdot\frac{\partial \text{net}_k}{\partial w_{kj}}\\
&=-(y_k-z_k)\cdot f'(\text{net}_k)\cdot y_j
\end{align}
$$
则梯度更新为：
$$
\begin{align}
w_{kj} \leftarrow w_{kj}+(y_k-z_k)\cdot f'(\text{net}_k)\cdot y_j
\end{align}
$$
输入到隐层的权值偏导：
$$
\begin{align}
\frac{\partial \mathcal{L}}{\partial w_{ji}} &= \frac{\partial \mathcal{L}}{\partial y_j}\cdot \frac{\partial y_j}{\partial \text{net}_j}\cdot \frac{\partial \text{net}_j}{\partial w_{ji}}\\
&= \sum_{k=1}^h\frac{\partial \mathcal{L}}{\partial \text{net}_k}\cdot\frac{\partial \text{net}_k}{\partial y_j}\cdot\frac{\partial y_j}{\partial \text{net}_j}\cdot \frac{\partial \text{net}_j}{\partial w_{ji}}\\
&= \sum_{k=1}^h -(y_k-z_k)\cdot f'(\text{net}_k)\cdot w_{kj}\cdot f'(\text{net}_j)\cdot x_i
\end{align}
$$
